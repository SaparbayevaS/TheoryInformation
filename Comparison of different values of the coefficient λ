import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd

class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc = nn.Linear(10, 3)

    def forward(self, x):
        return self.fc(x)

def entropy_regularization(logits):
    p = F.softmax(logits, dim=1)
    return -torch.mean(torch.sum(p * torch.log(p + 1e-8), dim=1))

def custom_loss_fn(outputs, targets, lambda_entropy=0.01):
    ce_loss = F.cross_entropy(outputs, targets)
    entropy_loss = entropy_regularization(outputs)
    return ce_loss + lambda_entropy * entropy_loss

def evaluate(model, loader):
    model.eval()
    correct = 0
    total = 0
    for x, y in loader:
        with torch.no_grad():
            preds = model(x).argmax(1)
            correct += (preds == y).sum().item()
            total += len(y)
    return correct / total

torch.manual_seed(0)
X = torch.randn(300, 10)
y = torch.randint(0, 3, (300,))
train_ds = TensorDataset(X[:200], y[:200])
test_ds = TensorDataset(X[200:], y[200:])
train_loader = DataLoader(train_ds, batch_size=32)
test_loader = DataLoader(test_ds, batch_size=32)

lambdas = [0.0, 0.01, 0.1, 0.5, 1.0]
results = []

for lambda_entropy in lambdas:
    model = SimpleModel()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    for epoch in range(10):
        model.train()
        for x_batch, y_batch in train_loader:
            optimizer.zero_grad()
            outputs = model(x_batch)
            loss = custom_loss_fn(outputs, y_batch, lambda_entropy)
            loss.backward()
            optimizer.step()

    train_acc = evaluate(model, train_loader)
    test_acc = evaluate(model, test_loader)
    overfit = train_acc - test_acc
    results.append((lambda_entropy, train_acc, test_acc, overfit))

df = pd.DataFrame(results, columns=['Lambda', 'Train Accuracy', 'Test Accuracy', 'Overfit'])
print(df)
